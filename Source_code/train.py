"""
C√ÄI ƒê·∫∂T TR∆Ø·ªöC:
# C√†i b·∫£n PyTorch CPU (v√¨ kh√¥ng c√≥ GPU CUDA)
pƒê√£ t√¨m th·∫•y torch: 2.9.0+cpuip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# C√†i Detectron2 t∆∞∆°ng th√≠ch
pip install 'git+https://github.com/facebookresearch/detectron2.git'

# Th∆∞ vi·ªán ph·ª•
pip install opencv-python pycocotools tqdm matplotlib
"""

"""
C·∫§U TR√öC D·ªÆ LI·ªÜU ƒê·ªÄ XU·∫§T:
dataset_root/
  train/ 
    image1.jpg
    ...
    _annotations.coco.json
  valid/
    image101.jpg
    ...
    _annotations.coco.json
"""

import os
import json
import argparse

from detectron2.engine import DefaultTrainer, default_setup, launch
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.data.datasets import register_coco_instances
from detectron2.evaluation import COCOEvaluator, DatasetEvaluators

# --- H√ÄM L·∫§Y S·ªê L·ªöP ---
def get_num_classes(coco_json_path):
    """ƒê·ªçc file JSON v√† tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng categories."""
    with open(coco_json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return len(data.get("categories", []))

# --- H√ÄM ƒêƒÇNG K√ù DATASET (ƒê√É S·ª¨A L·ªñI) ---
def register_datasets(train_dir, val_dir=None, train_name="my_train", val_name="my_val"):  # <<< val_dir l√† t√πy ch·ªçn
    """
    ƒêƒÉng k√Ω c√°c b·ªô d·ªØ li·ªáu train v√† (t√πy ch·ªçn) validation.
    """
    # <<< S·ª¨A L·ªñI LOGIC N·ªêI CHU·ªñI:
    train_json = os.path.join(train_dir, "_annotations.coco.json")

    # Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa file annotation
    if not os.path.isfile(train_json):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file _annotations.coco.json trong th∆∞ m·ª•c: {train_dir}")

    # ƒêƒÉng k√Ω dataset train
    register_coco_instances(train_name, {}, train_json, train_dir)

    val_json = None
    # <<< Ch·ªâ ƒëƒÉng k√Ω validation set n·∫øu val_dir ƒë∆∞·ª£c cung c·∫•p
    if val_dir:
        val_json = os.path.join(val_dir, "_annotations.coco.json")  # <<< S·ª¨A L·ªñI t∆∞∆°ng t·ª±
        if not os.path.isfile(val_json):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file _annotations.coco.json trong th∆∞ m·ª•c: {val_dir}")

        register_coco_instances(val_name, {}, val_json, val_dir)
    else:
        # N·∫øu kh√¥ng c√≥ val_dir, ƒë·∫∑t val_name l√† None
        val_name = None

    return train_json, val_json, train_name, val_name

# --- CLASS TRAINER T√ôY CH·ªàNH ---
class Trainer(DefaultTrainer):
    """
    Trainer t√πy ch·ªânh ƒë·ªÉ s·ª≠ d·ª•ng COCOEvaluator cho qu√° tr√¨nh ƒë√°nh gi√°.
    """
    @classmethod
    def build_evaluator(cls, cfg, dataset_name, output_folder=None):
        return COCOEvaluator(dataset_name, tasks=("bbox", "segm"), output_dir=output_folder)

# --- THI·∫æT L·∫¨P C·∫§U H√åNH ---
def setup_cfg(args, num_classes, train_name, val_name):  # val_name gi·ªù c√≥ th·ªÉ l√† None
    """Thi·∫øt l·∫≠p c·∫•u h√¨nh Detectron2 t·ª´ file YAML v√† c√°c tham s·ªë d√≤ng l·ªánh."""
    cfg = get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file(
        "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
    ))

    # C·∫•u h√¨nh dataset
    cfg.DATASETS.TRAIN = (train_name,)

    # <<< Ch·ªâ thi·∫øt l·∫≠p test set v√† chu k·ª≥ eval n·∫øu c√≥ val_name
    if val_name:
        cfg.DATASETS.TEST = (val_name,)
        cfg.TEST.EVAL_PERIOD = args.eval_period  # T·∫ßn su·∫•t ƒë√°nh gi√°
    else:
        cfg.DATASETS.TEST = ()
        cfg.TEST.EVAL_PERIOD = 0  # T·∫Øt ƒë√°nh gi√° trong qu√° tr√¨nh training

    cfg.DATALOADER.NUM_WORKERS = 2

    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")

    # C·∫•u h√¨nh Solver
    cfg.SOLVER.IMS_PER_BATCH = args.batch
    cfg.SOLVER.BASE_LR = args.base_lr
    cfg.SOLVER.MAX_ITER = args.max_iter
    cfg.SOLVER.STEPS = []

    # C·∫•u h√¨nh CPU training
    cfg.MODEL.DEVICE = "cpu"
    cfg.SOLVER.AMP.ENABLED = False

    cfg.OUTPUT_DIR = args.output
    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
    cfg.SEED = 42

    # C·∫•u h√¨nh Model
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5

    return cfg

# --- THAM S·ªê D√íNG L·ªÜNH ---
def parse_args():
    """ƒê·ªãnh nghƒ©a v√† ph√¢n t√≠ch c√°c tham s·ªë ƒë∆∞·ª£c truy·ªÅn v√†o t·ª´ d√≤ng l·ªánh."""
    p = argparse.ArgumentParser(description="Hu·∫•n luy·ªán m√¥ h√¨nh Mask R-CNN tr√™n CPU v·ªõi Detectron2")
    p.add_argument("--train_dir", type=str, required=True, help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c train (v√≠ d·ª•: data/train)")

    # <<< val_dir kh√¥ng c√≤n b·∫Øt bu·ªôc (required=False)
    p.add_argument("--val_dir", type=str, required=False, default=None, help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c valid (t√πy ch·ªçn)")

    p.add_argument("--output", type=str, default="output/maskrcnn_cpu", help="Th∆∞ m·ª•c ƒë·ªÉ l∆∞u checkpoints v√† logs")
    p.add_argument("--batch", type=int, default=1, help="K√≠ch th∆∞·ªõc batch size. N√™n ƒë·ªÉ nh·ªè (1 ho·∫∑c 2) cho CPU.")
    p.add_argument("--base_lr", type=float, default=0.0025, help="T·ªëc ƒë·ªô h·ªçc (learning rate)")
    p.add_argument("--max_iter", type=int, default=500, help="T·ªïng s·ªë v√≤ng l·∫∑p hu·∫•n luy·ªán")
    p.add_argument("--eval_period", type=int, default=100, help="Chu k·ª≥ ƒë√°nh gi√° tr√™n t·∫≠p valid (n·∫øu c√≥)")
    p.add_argument("--resume", action="store_true", help="C·ªù ƒë·ªÉ ti·∫øp t·ª•c training t·ª´ checkpoint cu·ªëi c√πng n·∫øu c√≥")
    p.add_argument("--num_gpus", type=int, default=0, help="S·ªë GPU s·ª≠ d·ª•ng (ƒë·∫∑t l√† 0 ƒë·ªÉ ch·ªâ d√πng CPU)")
    return p.parse_args()

# --- H√ÄM CH√çNH ---
def main(args):
    """H√†m ch√≠nh ƒëi·ªÅu ph·ªëi to√†n b·ªô quy tr√¨nh."""
    # ƒêƒÉng k√Ω datasets
    train_json, _, train_name, val_name = register_datasets(args.train_dir, args.val_dir)  # val_dir c√≥ th·ªÉ l√† None
    num_classes = get_num_classes(train_json)
    print(f"‚úÖ ƒê√£ ƒëƒÉng k√Ω dataset th√†nh c√¥ng. S·ªë l·ªõp (classes): {num_classes}")
    if not val_name:
        print("‚ÑπÔ∏è Kh√¥ng cung c·∫•p th∆∞ m·ª•c validation, b·ªè qua b∆∞·ªõc ƒë√°nh gi√°.")

    # Thi·∫øt l·∫≠p c·∫•u h√¨nh
    cfg = setup_cfg(args, num_classes, train_name, val_name)
    default_setup(cfg, args)

    # Kh·ªüi t·∫°o Trainer
    trainer = Trainer(cfg)
    trainer.resume_or_load(resume=args.resume)

    print("\nüöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh hu·∫•n luy·ªán...")
    trainer.train()
    print("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t.")

    # <<< Ch·ªâ ch·∫°y ƒë√°nh gi√° cu·ªëi c√πng N·∫æU c√≥ val_name
    if val_name:
        print("\nüß™ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...")
        evaluator = COCOEvaluator(val_name, tasks=("bbox", "segm"),
                                  output_dir=os.path.join(cfg.OUTPUT_DIR, "inference"))
        trainer.test(cfg, trainer.model, evaluators=DatasetEvaluators([evaluator]))
        print("‚úÖ ƒê√°nh gi√° ho√†n t·∫•t.")
    else:
        print("‚úÖ Ho√†n t·∫•t! (B·ªè qua ƒë√°nh gi√° cu·ªëi c√πng v√¨ kh√¥ng c√≥ validation set)")

# --- ƒêI·ªÇM B·∫ÆT ƒê·∫¶U ---
if __name__ == "__main__":
    args = parse_args()

    print(f"Ch·∫°y v·ªõi {args.num_gpus} GPUs (ch·∫ø ƒë·ªô CPU).")
    launch(
        main,
        num_gpus_per_machine=args.num_gpus,
        num_machines=1,
        machine_rank=0,
        dist_url="auto",
        args=(args,),
    )

"""
python3 train.py \
  --train_dir /home/hquan07/Bouding_box/Dataset/train \
  --batch 1 \
  --max_iter 1000
"""